
<h2>1. Machine Learning</h2>
<p>
Machine Learning (ML) is a subset of Artificial Intelligence that allows systems
to learn patterns from data and make predictions or decisions without being
explicitly programmed.
</p>

<p>
Instead of writing rules manually, we provide data and the algorithm learns the
relationship between inputs and outputs.
</p>

<hr>

<h2>2. Types of Machine Learning</h2>
<ul>
  <li>Supervised Learning</li>
  <li>Unsupervised Learning</li>
  <li>Reinforcement Learning</li>
</ul>

<p>
This document focuses on <b>Supervised Learning</b>.
</p>

<hr>

<h2>3. Supervised Learning</h2>
<p>
In supervised learning, the dataset contains both:
</p>
<ul>
  <li><b>Input features (X)</b></li>
  <li><b>Output labels (Y)</b></li>
</ul>

<p>
The model learns by comparing its predictions with the correct output.
</p>

<h3>Example</h3>
<table border="1" cellpadding="6">
  <tr>
    <th>Experience (X)</th>
    <th>Salary (Y)</th>
  </tr>
  <tr>
    <td>1 year</td>
    <td>3 LPA</td>
  </tr>
  <tr>
    <td>5 years</td>
    <td>10 LPA</td>
  </tr>
</table>

<hr>

<h2>4. Types of Supervised Learning</h2>

<h3>4.1 Classification</h3>
<p>
Classification problems have <b>categorical outputs</b>.
</p>

<ul>
  <li>Email spam detection (Spam / Not Spam)</li>
  <li>Disease prediction (Positive / Negative)</li>
  <li>Image classification (Cat / Dog)</li>
</ul>

<p><b>Common Algorithms:</b></p>
<ul>
  <li>Logistic Regression</li>
  <li>K-Nearest Neighbors (KNN)</li>
  <li>Decision Tree</li>
  <li>Support Vector Machine (SVM)</li>
</ul>

<hr>

<h3>4.2 Regression</h3>
<p>
Regression problems have <b>continuous numeric outputs</b>.
</p>

<ul>
  <li>House price prediction</li>
  <li>Salary prediction</li>
  <li>Temperature prediction</li>
</ul>

<p><b>Common Algorithms:</b></p>
<ul>
  <li>Linear Regression</li>
  <li>Polynomial Regression</li>
  <li>Ridge and Lasso Regression</li>
</ul>

<hr>

<h2>5. Simple Linear Regression</h2>
<p>
Simple Linear Regression models the relationship between one input variable (X)
and one output variable (Y).
</p>

<h3>Mathematical Equation</h3>
<p>
<b>y = mx + c</b>
</p>

<ul>
  <li><b>m</b> â€“ slope of the line</li>
  <li><b>c</b> â€“ intercept</li>
</ul>

<p>
The goal is to find the best-fitting straight line that minimizes prediction error.
</p>

<hr>

<h2>6. Trainâ€“Test Split</h2>
<p>
To evaluate model performance fairly, the dataset is split into:
</p>

<ul>
  <li><b>Training data</b> â€“ used to train the model</li>
  <li><b>Testing data</b> â€“ used to evaluate the model</li>
</ul>

<p>
A common split ratio is <b>80% training</b> and <b>20% testing</b>.
</p>

<pre>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
</pre>


<hr>

<h2>11. Workflow of Simple Linear Regression</h2>
<ol>
  <li>Collect data</li>
  <li>Split data into training and testing sets</li>
  <li>Train the model</li>
  <li>Minimize loss using gradient descent</li>
  <li>Make predictions</li>
  <li>Evaluate using MSE and RÂ² score</li>
</ol>

<hr>



<hr>

<h2>1. Linear Regression</h2>
<p>
Linear Regression is a basic and commonly used predictive analysis technique in
Machine Learning and Statistics. It is used to model the relationship between
a dependent variable (Y) and one or more independent variables (X).
</p>

<p>
The relationship between the variables is assumed to be linear, meaning it can
be represented using a straight line.
</p>

<hr>

<h2>2. Simple Linear Regression</h2>
<p>
Simple Linear Regression deals with only one independent variable (X) and one
dependent variable (Y). It assumes that a linear relationship exists between the
response variable and the explanatory variable.
</p>

<p>
This relationship is modeled using a linear surface called a <b>hyperplane</b>.
</p>

<h3>Hyperplane Explanation</h3>
<p>
A hyperplane is a subspace that has one dimension less than the space it exists in.
In simple linear regression:
</p>

<ul>
  <li>One dimension represents the input variable (X)</li>
  <li>One dimension represents the output variable (Y)</li>
</ul>

<p>
Thus, the total dimensions are two, and a hyperplane in two dimensions is simply
a <b>straight line</b>.
</p>

<hr>

<h2>3. Uses of Regression Analysis</h2>

<h3>1. Measuring Relationship Strength</h3>
<p>
Regression is used to identify the strength of the effect that independent
variables have on a dependent variable.
</p>

<p><b>Examples:</b></p>
<ul>
  <li>Dose and effect</li>
  <li>Sales and marketing spending</li>
  <li>Age and income</li>
</ul>

<h3>2. Measuring Impact of Change</h3>
<p>
Regression analysis helps understand how much the dependent variable changes
when the independent variable changes.
</p>

<p>
Example question: How much additional sales income is generated for every
â‚¹1000 spent on marketing?
</p>

<h3>3. Prediction of Future Values</h3>
<p>
Regression can be used to predict trends and future values.
</p>

<p><b>Examples:</b></p>
<ul>
  <li>Gold price after 6 months</li>
  <li>Future salary</li>
  <li>Trend estimation</li>
</ul>

<hr>

<h2>4. Line Equation in Linear Regression</h2>
<p>
The linear regression model is represented by the following equation:
</p>

<pre>
y = mx + c + e
</pre>

<ul>
  <li><b>y</b> â€“ Actual output value</li>
  <li><b>x</b> â€“ Input variable</li>
  <li><b>m</b> â€“ Slope of the line</li>
  <li><b>c</b> â€“ Intercept (value of y when x = 0)</li>
  <li><b>e</b> â€“ Error (difference between actual and predicted value)</li>
</ul>

<p>
The goal of linear regression is to find values of <b>m</b> and <b>c</b> such that
the error is minimized.
</p>

<hr>

<h2>5. Predicted Value and Error</h2>
<p>
The predicted value is given by <b>mx + c</b>. The error represents the difference
between the predicted value and the actual value.
</p>

<p>
The model always tries to minimize this error.
</p>

<hr>

<h2>6. Cost Function</h2>
<p>
The error in a regression model is also called the <b>cost function</b>.
</p>

<p>
The cost function measures the total squared error over the training dataset.
</p>

<h3>Cost Function Formula</h3>
<pre>
Cost Function = (Predicted Value - Actual Value)Â²
</pre>

<pre>
Cost Function = (y[i] - (m * x[i] + c))Â²
</pre>

<p>
Squaring the error ensures all errors are positive and large errors are penalized
more heavily.
</p>

<hr>

<h2>7. Gradient Descent</h2>
<p>
Gradient Descent is an optimization algorithm used to minimize the cost function.
It works iteratively to find the best values for model parameters.
</p>

<h3>Key Idea</h3>
<p>
The gradient of a function points in the direction of maximum increase.
To minimize the function, we move in the opposite direction of the gradient.
</p>

<h3>Steps in Gradient Descent</h3>
<ol>
  <li>Choose a random starting point for parameters</li>
  <li>Compute the gradient of the cost function</li>
  <li>Move parameters in the opposite direction of the gradient</li>
  <li>Repeat until the cost function stops decreasing</li>
</ol>

<hr>

<h2>8. Learning Rate (Î±)</h2>
<p>
The learning rate controls how much the parameters change in each step of
gradient descent.
</p>

<ul>
  <li>Too large â†’ may overshoot the minimum</li>
  <li>Too small â†’ very slow convergence</li>
  <li>Optimal value â†’ fast and stable learning</li>
</ul>

<hr>

<h2>9. Gradient Descent Mathematics (Slope Optimization)</h2>
<p>
For a simple model where only the slope is optimized:
</p>

<pre>
Cost Function = (y[i] - m * x[i])Â²
</pre>

<p>
Taking derivative with respect to m:
</p>

<pre>
d/dm (y[i] - m * x[i])Â²
= -2 (y[i] - m * x[i]) * x[i]
</pre>

<p>
This derivative is used to update the slope value.
</p>

<hr>

<h2>10. Direct Formula for Slope and Intercept</h2>

<h3>Slope (bâ‚)</h3>
<pre>
bâ‚ = Î£(x - xÌ„)(y - È³) / Î£(x - xÌ„)Â²
</pre>

<h3>Intercept (bâ‚€)</h3>
<pre>
bâ‚€ = È³ - bâ‚ * xÌ„
</pre>

<p>
Here, xÌ„ and È³ represent the mean values of X and Y respectively.
</p>

<hr>

<h2>11. RMSE (Root Mean Square Error)</h2>
<p>
RMSE measures the average magnitude of prediction errors.
</p>

<pre>
RMSE = âˆš(1/m Î£(y - Å·)Â²)
</pre>

<p>
Lower RMSE indicates a better model.
</p>

<hr>

<h2>12. RÂ² Score (Coefficient of Determination)</h2>
<p>
RÂ² score measures how well the regression line fits the data.
</p>

<pre>
RÂ² = 1 - Î£(y - Å·)Â² / Î£(y - È³)Â²
</pre>

<h3>Interpretation</h3>
<ul>
  <li>RÂ² â‰ˆ 1 â†’ Very good model</li>
  <li>RÂ² far from 1 â†’ Poor model</li>
</ul>

<hr>

<h2>13. Multiple Linear Regression</h2>
<p>
Multiple Linear Regression is an extension of simple linear regression where
there are multiple independent variables and one dependent variable.
</p>

<h3>Equation</h3>
<pre>
Y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™
</pre>

<p>
By introducing xâ‚€ = 1, the equation becomes:
</p>

<pre>
Y = Î²â‚€xâ‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™
</pre>

<hr>

<h2>14. Matrix Representation</h2>
<p>
The multiple linear regression equation can be written in matrix form:
</p>

<pre>
Y = Î²áµ€X
</pre>

<p>
This representation is efficient for handling large datasets.
</p>

<hr>

<h2>15. Cost Function in Multiple Linear Regression</h2>
<p>
The hypothesis function is:
</p>

<pre>
h<sub>Î²</sub>(x) = Î²áµ€x
</pre>

<p>
The cost function is defined as:
</p>

<pre>
J(Î²) = (1 / 2m) Î£(h<sub>Î²</sub>(xâ½â±â¾) - yâ½â±â¾)Â²
</pre>

<hr>

<h2>16. Gradient Descent for Multiple Linear Regression</h2>

<h3>Initialization</h3>
<p>
All Î² values are initialized to zero.
</p>

<h3>Update Rule</h3>
<pre>
Î²<sub>j</sub> := Î²<sub>j</sub> - Î± âˆ‚J(Î²)/âˆ‚Î²<sub>j</sub>
</pre>

<p>
After applying mathematics, the update equation becomes:
</p>

<pre>
Î²<sub>j</sub> := Î²<sub>j</sub> - Î± (1/m) Î£(h<sub>Î²</sub>(xâ½â±â¾) - yâ½â±â¾) x<sub>j</sub>â½â±â¾
</pre>

<hr>

<h2>17. Batch Gradient Descent</h2>
<p>
This method of gradient descent uses the entire dataset to update parameters
in each iteration. It is called <b>Batch Gradient Descent</b>.
</p>

<p>
It is stable and guarantees convergence for convex cost functions, but can be
computationally expensive for large datasets.
</p>

<hr>

<h2>Simple Linear Regression</h2>

<p>
<strong>Simple Linear Regression (SLR)</strong> is a statistical and machine learning technique
used to model the <strong>linear relationship</strong> between two variables:
</p>

<ul>
  <li><strong>One independent variable</strong> â€” often called feature (X)</li>
  <li><strong>One dependent variable</strong> â€” target (Y)</li>
</ul>

<p>
It assumes that changes in <strong>X</strong> result in proportional changes in <strong>Y</strong>,
and fits a straight line through the observed data to make predictions.
</p>

<hr>

<h2>ğŸ§  Model Equation</h2>

<p>The model is represented by the equation:</p>

<p style="font-size:18px;">
  <strong>y = Î²<sub>0</sub> + Î²<sub>1</sub>x + Îµ</strong>
</p>

<h4>Where:</h4>
<ul>
  <li><strong>y</strong> = dependent variable (value we want to predict)</li>
  <li><strong>x</strong> = independent variable (input)</li>
  <li><strong>Î²<sub>0</sub></strong> = intercept (predicted value of y when x = 0)</li>
  <li><strong>Î²<sub>1</sub></strong> = slope/coefficient (how strongly x affects y)</li>
  <li><strong>Îµ</strong> = error term (difference between actual and predicted values)</li>
</ul>

<p>
ğŸ‘‰ The goal is to choose values of <strong>Î²<sub>0</sub></strong> and <strong>Î²<sub>1</sub></strong>
so that the predicted line fits the data as closely as possible.
</p>

<h1>Multiple Linear Regression (MLR) & Deployment on Render</h1>

<hr>

<h2>PART 1ï¸âƒ£ What is Multiple Linear Regression (MLR)?</h2>

<h3>ğŸ”¹ Definition (Simple words)</h3>
<p>
<b>Multiple Linear Regression</b> is a supervised machine learning algorithm used to
<b>predict a numeric value</b> using <b>more than one input feature</b>.
</p>

<p>ğŸ‘‰ It is an extension of <b>Simple Linear Regression</b>.</p>

<hr>

<h3>ğŸ”¹ Mathematical Equation</h3>

<p><b>Simple Linear Regression:</b></p>
<p><code>y = mx + c</code></p>

<p><b>Multiple Linear Regression:</b></p>
<p>
<code>
y = b<sub>0</sub> + b<sub>1</sub>x<sub>1</sub> + b<sub>2</sub>x<sub>2</sub> + 
b<sub>3</sub>x<sub>3</sub> + ... + b<sub>n</sub>x<sub>n</sub>
</code>
</p>

<p><b>Where:</b></p>
<ul>
  <li><b>y</b> â†’ Predicted output (target)</li>
  <li><b>x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>...</b> â†’ Input features</li>
  <li><b>b<sub>0</sub></b> â†’ Intercept</li>
  <li><b>b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>...</b> â†’ Coefficients (weights)</li>
</ul>

<hr>

<h3>ğŸ”¹ Example (Real-world)</h3>

<p><b>Predict House Price using:</b></p>
<ul>
  <li>Area</li>
  <li>Number of bedrooms</li>
  <li>Location</li>
  <li>Age of house</li>
</ul>

<p><b>Mapping:</b></p>
<ul>
  <li>Area â†’ x<sub>1</sub></li>
  <li>Bedrooms â†’ x<sub>2</sub></li>
  <li>Location â†’ x<sub>3</sub></li>
  <li>Age â†’ x<sub>4</sub></li>
</ul>

<p>
The model learns how <b>each feature contributes</b> to the house price.
</p>

<hr>

<h3>ğŸ”¹ Why we use MLR?</h3>
<ul>
  <li>âœ… Uses multiple factors</li>
  <li>âœ… More accurate than Simple Linear Regression</li>
  <li>âœ… Widely used in salary, price, and demand prediction</li>
</ul>

<hr>

<h3>ğŸ”¹ Training an MLR Model (Concept)</h3>
<ol>
  <li>Load dataset</li>
  <li>Split data into <b>X (features)</b> and <b>y (target)</b></li>
  <li>Apply <code>train_test_split</code></li>
  <li>Train model using <code>LinearRegression()</code></li>
  <li>Evaluate using:
    <ul>
      <li>RÂ² Score</li>
      <li>Mean Squared Error (MSE)</li>
    </ul>
  </li>
</ol>

<hr



<h1>ğŸ“ˆ Polynomial Linear Regression</h1>

<h2>ğŸ” What is Polynomial Linear Regression?</h2>
<p>
Polynomial Linear Regression is a supervised machine learning regression technique
used when the relationship between the independent variable (X) and dependent
variable (Y) is <b>non-linear</b>, but the model is still <b>linear in parameters</b>.
</p>

<p><b>Key Idea:</b> The model is linear with respect to coefficients, but non-linear with respect to input features.</p>

<hr>

<h2>â“ Why Do We Need Polynomial Regression?</h2>
<p>
Simple Linear Regression assumes a straight-line relationship:
</p>

<p><b>y = mx + c</b></p>

<p>
However, real-world data often follows curved patterns such as:
</p>

<ul>
  <li>U-shaped curves</li>
  <li>Parabolic trends</li>
  <li>Complex growth patterns</li>
</ul>

<p>
In such cases, linear regression underfits the data.
Polynomial regression solves this problem by adding higher-degree terms.
</p>

<hr>

<h2>ğŸ§® Mathematical Representation</h2>

<p><b>Degree 2 (Quadratic):</b></p>
<p>y = bâ‚€ + bâ‚x + bâ‚‚xÂ²</p>

<p><b>Degree 3 (Cubic):</b></p>
<p>y = bâ‚€ + bâ‚x + bâ‚‚xÂ² + bâ‚ƒxÂ³</p>

<p><b>General form:</b></p>
<p>y = bâ‚€ + bâ‚x + bâ‚‚xÂ² + ... + bâ‚™xâ¿</p>

<p>
Even though higher powers of x exist, the equation is still linear
because coefficients are not multiplied together.
</p>

<hr>

<h2>ğŸ§  Why Is It Called Linear Regression?</h2>
<p>
It is called linear regression because the model is linear with respect to the
parameters (bâ‚€, bâ‚, bâ‚‚, ...), not the input features.
</p>

<hr>

<h2>âš™ï¸ How Polynomial Regression Works</h2>

<ol>
  <li>Start with the original feature X</li>
  <li>Create polynomial features (xÂ², xÂ³, ...)</li>
  <li>Apply linear regression on the transformed features</li>
</ol>

<hr>

<h2>ğŸ§‘â€ğŸ’» Python Implementation (scikit-learn)</h2>

<pre>
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, y)

y_pred = model.predict(X_poly)
</pre>

<hr>

<h2>ğŸ¯ Choosing the Polynomial Degree</h2>

<table border="1" cellpadding="6">
  <tr>
    <th>Degree</th>
    <th>Effect</th>
  </tr>
  <tr>
    <td>Low</td>
    <td>Underfitting</td>
  </tr>
  <tr>
    <td>Optimal</td>
    <td>Good bias-variance balance</td>
  </tr>
  <tr>
    <td>High</td>
    <td>Overfitting</td>
  </tr>
</table>

<hr>

<h2>âš ï¸ Overfitting Problem</h2>
<p>
High-degree polynomials may perfectly fit training data but perform poorly
on unseen data.
</p>

<p><b>Solutions:</b></p>
<ul>
  <li>Cross-validation</li>
  <li>Regularization (Ridge, Lasso)</li>
  <li>Choosing optimal degree</li>
</ul>

<hr>

<h2>âœ… Advantages</h2>
<ul>
  <li>Models non-linear relationships</li>
  <li>Easy to implement</li>
  <li>Flexible and powerful</li>
</ul>

<h2>âŒ Disadvantages</h2>
<ul>
  <li>Prone to overfitting</li>
  <li>Sensitive to outliers</li>
  <li>Poor extrapolation</li>
</ul>

<hr>

<h2>ğŸ†š Linear vs Polynomial Regression</h2>

<table border="1" cellpadding="6">
  <tr>
    <th>Feature</th>
    <th>Linear</th>
    <th>Polynomial</th>
  </tr>
  <tr>
    <td>Relationship</td>
    <td>Straight line</td>
    <td>Curved</td>
  </tr>
  <tr>
    <td>Complexity</td>
    <td>Low</td>
    <td>Higher</td>
  </tr>
  <tr>
    <td>Accuracy on non-linear data</td>
    <td>Low</td>
    <td>High</td>
  </tr>
</table>

<hr>

<h2>ğŸ“Œ Real-World Applications</h2>
<ul>
  <li>Salary prediction</li>
  <li>House price prediction</li>
  <li>Growth trend analysis</li>
  <li>Engineering and physics models</li>
</ul>

<hr>

<h2>ğŸ“ Summary</h2>
<p>
Polynomial Linear Regression extends linear regression by introducing
polynomial features to capture non-linear patterns while keeping the model linear.
Choosing the correct degree is critical for good performance.
</p>


<h1>ğŸ“Š RÂ² Score and Adjusted RÂ² Score</h1>

<h2>ğŸ” What is RÂ² Score?</h2>
<p>
RÂ² Score (Coefficient of Determination) measures how well a regression model
explains the variation in the dependent variable (Y).
</p>

<p><b>In simple words:</b></p>
<p>
It tells us what percentage of the total variation in the target variable
is explained by the model.
</p>

<hr>

<h2>ğŸ§  Intuition Behind RÂ² Score</h2>
<p>
If your model predicts values close to the actual data points,
the RÂ² score will be high.
</p>

<p>
For example, an RÂ² score of <b>0.85</b> means:
</p>
<ul>
  <li>85% of the variation in Y is explained by the model</li>
  <li>15% remains unexplained</li>
</ul>

<hr>

<h2>ğŸ“ Formula of RÂ² Score</h2>

<p>
<b>RÂ² = 1 âˆ’ (SS<sub>res</sub> / SS<sub>tot</sub>)</b>
</p>

<p><b>Where:</b></p>
<ul>
  <li><b>SS<sub>tot</sub></b> â€“ Total Sum of Squares (total variation in Y)</li>
  <li><b>SS<sub>res</sub></b> â€“ Residual Sum of Squares (prediction error)</li>
</ul>

<hr>

<h2>ğŸ“Š RÂ² Score Interpretation</h2>

<table border="1" cellpadding="6">
  <tr>
    <th>RÂ² Value</th>
    <th>Interpretation</th>
  </tr>
  <tr>
    <td>1.0</td>
    <td>Perfect model</td>
  </tr>
  <tr>
    <td>0.9</td>
    <td>Very good fit</td>
  </tr>
  <tr>
    <td>0.7</td>
    <td>Good fit</td>
  </tr>
  <tr>
    <td>0.0</td>
    <td>No explanatory power</td>
  </tr>
  <tr>
    <td>&lt; 0</td>
    <td>Worse than predicting mean</td>
  </tr>
</table>

<hr>

<h2>âš ï¸ Limitation of RÂ² Score</h2>
<p>
RÂ² score always increases when new features are added,
even if those features are useless.
</p>

<p>
This makes RÂ² unreliable for Multiple Linear Regression.
</p>

<hr>

<h2>ğŸ“‰ What is Adjusted RÂ² Score?</h2>
<p>
Adjusted RÂ² improves RÂ² by penalizing unnecessary independent variables.
</p>

<p>
It increases only when a new feature actually improves the model.
</p>

<hr>

<h2>ğŸ“ Formula of Adjusted RÂ²</h2>

<p>
<b>
Adjusted RÂ² = 1 âˆ’ [(1 âˆ’ RÂ²) Ã— (n âˆ’ 1) / (n âˆ’ p âˆ’ 1)]
</b>
</p>

<p><b>Where:</b></p>
<ul>
  <li><b>n</b> = number of observations</li>
  <li><b>p</b> = number of independent variables</li>
  <li><b>RÂ²</b> = RÂ² score</li>
</ul>

<hr>

<h2>ğŸ§  Why Adjusted RÂ² is Important</h2>
<ul>
  <li>Penalizes irrelevant features</li>
  <li>Prevents overfitting</li>
  <li>More reliable for multiple regression</li>
</ul>

<hr>

<h2>ğŸ†š RÂ² Score vs Adjusted RÂ² Score</h2>

<table border="1" cellpadding="6">
  <tr>
    <th>Feature</th>
    <th>RÂ² Score</th>
    <th>Adjusted RÂ² Score</th>
  </tr>
  <tr>
    <td>Penalizes extra features</td>
    <td>No</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>Always increases</td>
    <td>Yes</td>
    <td>No</td>
  </tr>
  <tr>
    <td>Best for</td>
    <td>Simple Linear Regression</td>
    <td>Multiple Linear Regression</td>
  </tr>
  <tr>
    <td>Reliability</td>
    <td>Medium</td>
    <td>High</td>
  </tr>
</table>

<hr>

<h2>ğŸ§ª Python Example</h2>

<pre>
from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred)
print("R2 Score:", r2)

n = X_test.shape[0]
p = X_test.shape[1]

adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print("Adjusted R2:", adjusted_r2)
</pre>

<hr>

<h2>ğŸ“Œ When to Use Which?</h2>

<ul>
  <li>Use <b>RÂ²</b> for Simple Linear Regression</li>
  <li>Use <b>Adjusted RÂ²</b> for Multiple Linear Regression</li>
  <li>Use Adjusted RÂ² for model comparison</li>
</ul>

<hr>

<h2>ğŸ“ Summary</h2>
<p>
RÂ² score measures how well a regression model fits the data,
while Adjusted RÂ² provides a more reliable measure by considering
the number of features used in the model.
</p>

<h1>ğŸ¯ Regularization in Machine Learning</h1>

<h2>ğŸ” What is Regularization?</h2>
<p>
Regularization is a technique used to prevent <b>overfitting</b> in machine learning models
by adding a <b>penalty term</b> to the loss function.
</p>

<p>
In simple words, regularization discourages the model from learning
complex patterns by forcing the coefficients to remain small.
</p>

<hr>

<h2>â“ Why Do We Need Regularization?</h2>
<p>
Overfitting occurs when a model:
</p>
<ul>
  <li>Performs very well on training data</li>
  <li>Performs poorly on unseen test data</li>
  <li>Learns noise instead of the real pattern</li>
</ul>

<p>
This problem is common in:
</p>
<ul>
  <li>Polynomial Regression</li>
  <li>Multiple Linear Regression</li>
  <li>High-dimensional datasets</li>
</ul>

<hr>

<h2>ğŸ§  Intuition Behind Regularization</h2>
<p>
Large coefficient values make the model overly sensitive to small changes in input data.
Regularization penalizes large coefficients and keeps the model simple and stable.
</p>

<hr>

<h2>ğŸ§® Cost Function Without Regularization</h2>
<p>
<b>Loss = Î£ (y âˆ’ Å·)Â²</b>
</p>

<p>
This minimizes prediction error but does not control model complexity.
</p>

<hr>

<h2>â• Cost Function With Regularization</h2>
<p>
<b>Loss = Î£ (y âˆ’ Å·)Â² + Penalty Term</b>
</p>

<p>
The penalty term depends on the type of regularization used.
</p>

<hr>

<h2>ğŸ”¥ Types of Regularization</h2>
<ul>
  <li>Ridge Regression (L2)</li>
  <li>Lasso Regression (L1)</li>
  <li>Elastic Net</li>
</ul>

<hr>

<h2>1ï¸âƒ£ Ridge Regression (L2 Regularization)</h2>
<p>
Ridge regression adds the square of the coefficients as a penalty term.
</p>

<p><b>Formula:</b></p>
<p>
Loss = Î£ (y âˆ’ Å·)Â² + Î» Î£ wÂ²
</p>

<ul>
  <li>Shrinks coefficients toward zero</li>
  <li>Does not eliminate features</li>
  <li>Handles multicollinearity well</li>
</ul>

<hr>

<h2>2ï¸âƒ£ Lasso Regression (L1 Regularization)</h2>
<p>
Lasso regression adds the absolute value of the coefficients as a penalty.
</p>

<p><b>Formula:</b></p>
<p>
Loss = Î£ (y âˆ’ Å·)Â² + Î» Î£ |w|
</p>

<ul>
  <li>Shrinks coefficients</li>
  <li>Can make coefficients exactly zero</li>
  <li>Performs feature selection</li>
</ul>

<hr>

<h2>3ï¸âƒ£ Elastic Net Regularization</h2>
<p>
Elastic Net is a combination of Ridge and Lasso regularization.
</p>

<p><b>Formula:</b></p>
<p>
Loss = Î£ (y âˆ’ Å·)Â² + Î»â‚ Î£ |w| + Î»â‚‚ Î£ wÂ²
</p>

<ul>
  <li>Handles multicollinearity</li>
  <li>Performs feature selection</li>
  <li>More stable than Lasso alone</li>
</ul>

<hr>

<h2>ğŸ›ï¸ Role of Lambda (Î»)</h2>
<p>
Lambda controls the strength of regularization.
</p>

<table border="1" cellpadding="6">
  <tr>
    <th>Î» Value</th>
    <th>Effect</th>
  </tr>
  <tr>
    <td>0</td>
    <td>No regularization</td>
  </tr>
  <tr>
    <td>Small</td>
    <td>Slight penalty</td>
  </tr>
  <tr>
    <td>Large</td>
    <td>Strong penalty</td>
  </tr>
  <tr>
    <td>Very large</td>
    <td>Underfitting</td>
  </tr>
</table>

<hr>

<h2>âš–ï¸ Biasâ€“Variance Tradeoff</h2>
<p>
Regularization slightly increases bias but significantly reduces variance,
resulting in better generalization.
</p>

<hr>

<h2>ğŸ§ª Python Example (scikit-learn)</h2>

<pre>
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Elastic Net
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)
</pre>

<hr>

<h2>ğŸ†š Regularization Comparison</h2>

<table border="1" cellpadding="6">
  <tr>
    <th>Feature</th>
    <th>Ridge</th>
    <th>Lasso</th>
    <th>Elastic Net</th>
  </tr>
  <tr>
    <td>Penalty Type</td>
    <td>L2</td>
    <td>L1</td>
    <td>L1 + L2</td>
  </tr>
  <tr>
    <td>Feature Selection</td>
    <td>No</td>
    <td>Yes</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>Handles Multicollinearity</td>
    <td>Yes</td>
    <td>No</td>
    <td>Yes</td>
  </tr>
</table>

<hr>

<h2>ğŸ“Œ When to Use Regularization?</h2>
<ul>
  <li>When the model overfits</li>
  <li>When dataset has many features</li>
  <li>When using polynomial features</li>
  <li>When multicollinearity exists</li>
</ul>

<hr>

<h2>ğŸ“ Summary</h2>
<p>
Regularization helps build simpler, more generalizable models by penalizing
large coefficients. Ridge, Lasso, and Elastic Net are the most commonly used
regularization techniques in regression problems.
</p>
